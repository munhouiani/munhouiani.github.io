<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/css/all.min.css" integrity="sha256-/4UQcSmErDzPCMAiuOiWPVVsNN2s3ZY/NsmXNcj0IFc=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"blog.munhou.com","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.15.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":false,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="What Are Policy Gradient MethodsPolicy gradient methods are a class of reinforcement learning algorithms used for optimizing parametrized policies by gradient descent to maximize the expected return,">
<meta property="og:type" content="article">
<meta property="og:title" content="Policy Gradient Methods">
<meta property="og:url" content="https://blog.munhou.com/2023/03/01/Policy-Gradient-Methods/index.html">
<meta property="og:site_name" content="Mun Hou&#39;s Blog">
<meta property="og:description" content="What Are Policy Gradient MethodsPolicy gradient methods are a class of reinforcement learning algorithms used for optimizing parametrized policies by gradient descent to maximize the expected return,">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-03-01T13:22:00.000Z">
<meta property="article:modified_time" content="2023-03-01T22:06:47.095Z">
<meta property="article:author" content="Mun Hou">
<meta property="article:tag" content="Reinforcement Learning">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://blog.munhou.com/2023/03/01/Policy-Gradient-Methods/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://blog.munhou.com/2023/03/01/Policy-Gradient-Methods/","path":"2023/03/01/Policy-Gradient-Methods/","title":"Policy Gradient Methods"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Policy Gradient Methods | Mun Hou's Blog</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-61451029-2"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-61451029-2","only_pageview":false}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Mun Hou's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#What-Are-Policy-Gradient-Methods"><span class="nav-number">1.</span> <span class="nav-text">What Are Policy Gradient Methods</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#What-is-a-Policy"><span class="nav-number">2.</span> <span class="nav-text">What is a Policy</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Commonly-Used-Gradient-Estimator"><span class="nav-number">3.</span> <span class="nav-text">Commonly Used Gradient Estimator</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Use-Gradient-Ascent-to-Update-Policy"><span class="nav-number">4.</span> <span class="nav-text">Use Gradient Ascent to Update Policy</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Implementation-Tip-1-Add-a-Baseline"><span class="nav-number">5.</span> <span class="nav-text">Implementation Tip 1: Add a Baseline</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Implementation-Tip-2-Assign-Suitable-Credit"><span class="nav-number">6.</span> <span class="nav-text">Implementation Tip 2: Assign Suitable Credit</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Advantage-Function"><span class="nav-number">7.</span> <span class="nav-text">Advantage Function</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Mun Hou</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">12</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/munhouiani" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;munhouiani" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:munhou2022+blog@gmail.com" title="E-Mail → mailto:munhou2022+blog@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blog.munhou.com/2023/03/01/Policy-Gradient-Methods/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mun Hou">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mun Hou's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Policy Gradient Methods | Mun Hou's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Policy Gradient Methods
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-03-01 21:22:00" itemprop="dateCreated datePublished" datetime="2023-03-01T21:22:00+08:00">2023-03-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-03-02 06:06:47" itemprop="dateModified" datetime="2023-03-02T06:06:47+08:00">2023-03-02</time>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2023/03/01/Policy-Gradient-Methods/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2023/03/01/Policy-Gradient-Methods/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h2 id="What-Are-Policy-Gradient-Methods"><a href="#What-Are-Policy-Gradient-Methods" class="headerlink" title="What Are Policy Gradient Methods"></a>What Are Policy Gradient Methods</h2><p>Policy gradient methods are a class of reinforcement learning algorithms used for <em>optimizing parametrized policies by gradient descent to maximize the expected return</em>, which is the cumulative reward received over a long-term horizon. These methods are based on updating the policy parameters such that the probabilities of actions leading to higher returns are increased. In comparison, the probabilities of actions leading to lower returns are decreased.</p>
<p>Iteratively optimizing the policy using this approach makes it possible to arrive at an optimal policy that maximizes the expected return. This technique has proven effective in solving complex decision-making problems in various fields, including robotics, game playing, and natural language processing.</p>
<span id="more"></span>

<h2 id="What-is-a-Policy"><a href="#What-is-a-Policy" class="headerlink" title="What is a Policy"></a>What is a Policy</h2><p>In reinforcement learning, a policy, $\pi$, is a function that maps an agent’s current observation to a distribution over actions that the agent can take. In the context of neural network-based policies, the policy function $\pi_{\theta}$ is represented by a network with tunable parameters denoted by $\theta$. The input to the policy network is typically a vector or matrix representing the agent’s observation. The output is a probability distribution over possible actions, with each action corresponding to a neuron in the output layer.</p>
<h2 id="Commonly-Used-Gradient-Estimator"><a href="#Commonly-Used-Gradient-Estimator" class="headerlink" title="Commonly Used Gradient Estimator"></a>Commonly Used Gradient Estimator</h2><p>Given an expected return</p>
<p>$$<br>\hat{R} _{\theta} &#x3D; \sum _{\tau} R(t) \pi _{\theta} (t)<br>$$</p>
<p>Then the gradient</p>
<p>$$<br>\begin{aligned}<br>\nabla \hat{R} _{\theta} &amp;&#x3D; \hat{g} \\<br>&amp;&#x3D; \sum _{\tau} R( \tau ) \nabla \pi _{\theta} (\tau) \\<br>&amp;&#x3D; \sum _{\tau} R(\tau) \pi _{\theta} \frac{\nabla \pi _{\theta} (\tau)} {\pi _{\theta}( \tau)} \\<br>&amp;&#x3D; \sum _{\tau} R(\tau) \pi _{\theta} (\tau) \nabla \log \pi _{\theta} (\tau) \\<br>&amp;&#x3D; \hat{ \mathbb{E} } _{\tau \sim \pi _{\theta} (\tau)} \bigg[ R(\tau) \nabla \log \pi _{\theta} (\tau) \bigg] \\<br>&amp; \approx \frac{1}{N} \sum ^{N} _{n&#x3D;1} R(\tau^{n}) \nabla \log \pi _{\theta}(\tau ^n) \\<br>&amp;&#x3D; \frac{1}{N} \sum^{N} _{n&#x3D;1} \sum^{T _{n}} _{t&#x3D;1} R(\tau ^n) \nabla \log \pi _{\theta} (a^n_t | s^n_t)<br>\end{aligned}<br>$$</p>
<p>where</p>
<ul>
<li>$\tau$ is a trajectory</li>
<li>$\pi_{\theta}$ is a stochastic policy</li>
<li>$a^n_t$ is the taken action of the $n$-th trajectory at time $t$</li>
<li>$s^n_t$ is the state of the $n$-th trajectory at time $t$</li>
<li>$\nabla_{\pi_{\theta}}$ is the gradient operator of the policy $\pi$ with respect to $\theta$</li>
<li>$\hat{ \mathbb{E}} _{\tau \sim \pi _{\theta}(\tau)}[\dots]$ is the empirical average expectation over a finite batch of samples in an algorithm that alternates between sampling and optimization</li>
<li>$\frac{1}{N} \sum^{N} _{n&#x3D;1} R(\tau^{n}) \nabla \log \pi _{\theta}(\tau^n)$ is the approximation of the $\hat {\mathbb{E}} _{\tau \sim \pi _{\theta}(\tau)}[\dots]$ by sampling since it is impossible to collect all possible state and action pairs.</li>
</ul>
<p>The $\log$ term is a log-derivative trick that allows us to rewrite a derivative of a function as a product of its logarithm and its derivative and hence, simplify the computation of gradients for stochastic policies that are defined by probability distributions.</p>
<h2 id="Use-Gradient-Ascent-to-Update-Policy"><a href="#Use-Gradient-Ascent-to-Update-Policy" class="headerlink" title="Use Gradient Ascent to Update Policy"></a>Use Gradient Ascent to Update Policy</h2><p>We use gradient ascent to update the parameters $\theta$ of the policy $\pi_{\theta}$ using policy gradient methods. The reason for this is that we want to encourage the taken action $\pi_{\theta}(a^n_t|s^n_t)$ that results in a larger return $R(\tau^n)$. The update rule for the parameters is given by:</p>
<p>$$<br>\theta \leftarrow \theta + \eta \nabla \hat{R}_{\theta}<br>$$</p>
<p>where</p>
<p>$$<br>\nabla \hat{R} _{\theta} \approx \frac{1}{N} \sum^{N} _{n&#x3D;1} \sum^{T _{n}} _{t&#x3D;1} R(\tau ^n) \nabla \log \pi _{\theta} (a^n_t | s^n_t)<br>$$</p>
<p>The update rule nudges the parameters toward higher expected returns, improving the policy over time.</p>
<h2 id="Implementation-Tip-1-Add-a-Baseline"><a href="#Implementation-Tip-1-Add-a-Baseline" class="headerlink" title="Implementation Tip 1: Add a Baseline"></a>Implementation Tip 1: Add a Baseline</h2><p>Given that</p>
<p>$$<br>\nabla \hat{R} _{\theta} \approx \frac{1}{N} \sum^{N} _{n&#x3D;1} \sum^{T _{n}} _{t&#x3D;1} R(\tau^n) \nabla \log \pi _{\theta} (a^n_t | s^n_t)<br>$$</p>
<p>We will subtract the return with a baseline $b$, as for every trajectory $\tau^n$, the return $R(\tau^n)$ may always be positive. Subtracting a baseline $b$ gives the equation a positive and negative value.</p>
<p>$$<br>\nabla \hat{R} _{\theta} \approx \frac{1}{N} \sum^{N} _{n&#x3D;1} \sum^{T _{n}} _{t&#x3D;1} (R(\tau^n) - b) \nabla \log \pi _{\theta} (a^n_t | s^n_t)<br>$$</p>
<p>The selection of $b$ varies by choice. A straightforward implementation uses the expected value of the return $R(\tau)$.</p>
<p>$$<br>b \approx \mathbb{E}[R(\tau)]<br>$$</p>
<h2 id="Implementation-Tip-2-Assign-Suitable-Credit"><a href="#Implementation-Tip-2-Assign-Suitable-Credit" class="headerlink" title="Implementation Tip 2: Assign Suitable Credit"></a>Implementation Tip 2: Assign Suitable Credit</h2><p>Given that</p>
<p>$$<br>\nabla \hat{R} _{\theta} \approx \frac{1}{N} \sum^{N} _{n&#x3D;1} \sum^{T _{n}} _{t&#x3D;1} R(\tau^n) \nabla \log \pi _{\theta} (a^n_t | s^n_t)<br>$$</p>
<p>For the result of each specified state-action pair $\pi_{\theta} (a^n_t | s^n_t)$, instead of weighting by the return of a trajectory $R(\tau^n)$, it is more suitable to weight it by a discounted term, i.e., a discounted total reward after this state-action pair.</p>
<p>$$<br>\nabla \hat{R} _{\theta} \approx \frac{1}{N} \sum^{N} _{n&#x3D;1} \sum ^{T _{n}} _{t&#x3D;1} \sum ^{T_n} _{t’&#x3D;t} \gamma ^{t’ - t} r^n _{t’} \nabla \log \pi _{\theta} (a^n_t | s^n_t)<br>$$</p>
<p>where<br>$\gamma$ is a discount factor, $\gamma &lt; 1$, usually set to $0.9$ or $0.99$</p>
<h2 id="Advantage-Function"><a href="#Advantage-Function" class="headerlink" title="Advantage Function"></a>Advantage Function</h2><p>Combining implementation tip 1 and 2, we called $\sum ^{T_n} _{t’&#x3D;t} \gamma ^{t’ - t} r^n _{t’} - b$ an <em>advantage function</em>, denoted as</p>
<p>$$<br>A^{\theta}(s_t, a_t)<br>$$</p>
<p>This advantage function can be estimated by a <em>“critic”</em>. The meaning of an advantage function is to tell how good it is if we take $a_t$ other than other actions at $s_t$</p>
<p>Therefore, the gradient term becomes</p>
<p>$$<br>\begin{aligned}<br>\nabla \hat{R} _{\theta} &amp;&#x3D; \hat{g} \\<br>&amp;&#x3D; \hat{\mathbb{E}} _{\tau \sim \pi _{\theta}(\tau)} \bigg[ A^{\theta}(\tau) \nabla \log \pi _{\theta} (\tau) \bigg] \\<br>&amp;\approx \frac{1}{N} \sum^{N} _{n&#x3D;1} \sum^{T _{n}} _{t&#x3D;1} A ^{\theta} (s^n_t, a^n_t) \nabla \log \pi _{\theta} (a^n_t | s^n_t)<br>\end{aligned}<br>$$</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Reinforcement-Learning/" rel="tag"># Reinforcement Learning</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/03/01/Importance-Sampling/" rel="prev" title="Importance Sampling">
                  <i class="fa fa-chevron-left"></i> Importance Sampling
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/03/01/Off-Policy-Gradient-Methods/" rel="next" title="Off-Policy Gradient Methods">
                  Off-Policy Gradient Methods <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    
  <div class="comments" id="disqus_thread">
    <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
  </div>
  
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Mun Hou</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/munhouiani" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  




  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="disqus" type="application/json">{"enable":true,"shortname":"munhousblog","count":true,"i18n":{"disqus":"disqus"}}</script>
<script src="/js/third-party/comments/disqus.js"></script>

</body>
</html>
