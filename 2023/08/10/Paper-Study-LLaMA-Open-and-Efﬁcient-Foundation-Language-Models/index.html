<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"blog.munhou.com","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.18.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":false,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="LLaMA Open and Efﬁcient Foundation Language Models📖 AbstractWe introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of token">
<meta property="og:type" content="article">
<meta property="og:title" content="Paper Study: LLaMA Open and Efﬁcient Foundation Language Models">
<meta property="og:url" content="https://blog.munhou.com/2023/08/10/Paper-Study-LLaMA-Open-and-Ef%EF%AC%81cient-Foundation-Language-Models/index.html">
<meta property="og:site_name" content="Mun Hou&#39;s Blog">
<meta property="og:description" content="LLaMA Open and Efﬁcient Foundation Language Models📖 AbstractWe introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of token">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://blog.munhou.com/images/llama/CleanShot2023-08-07at231107.png">
<meta property="og:image" content="https://blog.munhou.com/images/llama/CleanShot2023-08-07at235133.png">
<meta property="og:image" content="https://blog.munhou.com/images/llama/CleanShot2023-08-08at001349.png">
<meta property="og:image" content="https://blog.munhou.com/images/llama/CleanShot2023-08-08at002024.png">
<meta property="og:image" content="https://blog.munhou.com/images/llama/CleanShot2023-08-08at001437.png">
<meta property="og:image" content="https://blog.munhou.com/images/llama/CleanShot2023-08-08at001547.png">
<meta property="og:image" content="https://blog.munhou.com/images/llama/CleanShot2023-08-08at001621.png">
<meta property="og:image" content="https://blog.munhou.com/images/llama/CleanShot2023-08-08at001639.png">
<meta property="og:image" content="https://blog.munhou.com/images/llama/CleanShot2023-08-08at001710.png">
<meta property="og:image" content="https://blog.munhou.com/images/llama/CleanShot2023-08-08at001740.png">
<meta property="og:image" content="https://blog.munhou.com/images/llama/CleanShot2023-08-08at001831.png">
<meta property="og:image" content="https://blog.munhou.com/images/llama/CleanShot2023-08-08at001912.png">
<meta property="og:image" content="https://blog.munhou.com/images/llama/CleanShot2023-08-08at002122.png">
<meta property="og:image" content="https://blog.munhou.com/images/llama/CleanShot2023-08-08at002142.png">
<meta property="og:image" content="https://blog.munhou.com/images/llama/CleanShot2023-08-08at002204.png">
<meta property="og:image" content="https://blog.munhou.com/images/llama/CleanShot2023-08-08at002223.png">
<meta property="article:published_time" content="2023-08-10T23:43:00.000Z">
<meta property="article:modified_time" content="2023-08-12T00:05:39.146Z">
<meta property="article:author" content="Mun Hou">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="Paper Study">
<meta property="article:tag" content="Large Language Model">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog.munhou.com/images/llama/CleanShot2023-08-07at231107.png">


<link rel="canonical" href="https://blog.munhou.com/2023/08/10/Paper-Study-LLaMA-Open-and-Ef%EF%AC%81cient-Foundation-Language-Models/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://blog.munhou.com/2023/08/10/Paper-Study-LLaMA-Open-and-Ef%EF%AC%81cient-Foundation-Language-Models/","path":"2023/08/10/Paper-Study-LLaMA-Open-and-Efﬁcient-Foundation-Language-Models/","title":"Paper Study: LLaMA Open and Efﬁcient Foundation Language Models"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Paper Study: LLaMA Open and Efﬁcient Foundation Language Models | Mun Hou's Blog</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-61451029-2"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-61451029-2","only_pageview":false}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Mun Hou's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#LLaMA-Open-and-Ef%EF%AC%81cient-Foundation-Language-Models"><span class="nav-number">1.</span> <span class="nav-text">LLaMA Open and Efﬁcient Foundation Language Models</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%F0%9F%93%96-Abstract"><span class="nav-number">1.1.</span> <span class="nav-text">📖 Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%F0%9F%8E%AF-Goal"><span class="nav-number">1.2.</span> <span class="nav-text">🎯 Goal</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%F0%9F%A4%94-Hypothesis"><span class="nav-number">1.3.</span> <span class="nav-text">🤔 Hypothesis</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%F0%9F%92%A1-Methodology"><span class="nav-number">1.4.</span> <span class="nav-text">💡 Methodology</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%F0%9F%93%8A-Result-s"><span class="nav-number">1.5.</span> <span class="nav-text">📊 Result(s)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Training-Loss"><span class="nav-number">1.5.1.</span> <span class="nav-text">Training Loss</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Evolution-of-Performance"><span class="nav-number">1.5.2.</span> <span class="nav-text">Evolution of Performance</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Zero-Shot-and-Few-Shot-Performance"><span class="nav-number">1.5.3.</span> <span class="nav-text">Zero-Shot and Few-Shot Performance</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Instruction-Finetuning-Result"><span class="nav-number">1.5.4.</span> <span class="nav-text">Instruction Finetuning Result</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Bias-Toxicity-and-Misinformation"><span class="nav-number">1.5.5.</span> <span class="nav-text">Bias, Toxicity and Misinformation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%F0%9F%94%91-Summary-of-Key-Points"><span class="nav-number">1.6.</span> <span class="nav-text">🔑 Summary of Key Points</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E2%9D%97-Significance"><span class="nav-number">1.7.</span> <span class="nav-text">❗ Significance</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%F0%9F%99%8B%E2%80%8D%E2%99%82%EF%B8%8F-Questions-and-Answers"><span class="nav-number">1.8.</span> <span class="nav-text">🙋‍♂️ Questions and Answers</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Mun Hou</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">14</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/munhouiani" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;munhouiani" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:munhou2022+blog@gmail.com" title="E-Mail → mailto:munhou2022+blog@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blog.munhou.com/2023/08/10/Paper-Study-LLaMA-Open-and-Ef%EF%AC%81cient-Foundation-Language-Models/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mun Hou">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mun Hou's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Paper Study: LLaMA Open and Efﬁcient Foundation Language Models | Mun Hou's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Paper Study: LLaMA Open and Efﬁcient Foundation Language Models
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-08-11 07:43:00" itemprop="dateCreated datePublished" datetime="2023-08-11T07:43:00+08:00">2023-08-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-08-12 08:05:39" itemprop="dateModified" datetime="2023-08-12T08:05:39+08:00">2023-08-12</time>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2023/08/10/Paper-Study-LLaMA-Open-and-Ef%EF%AC%81cient-Foundation-Language-Models/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2023/08/10/Paper-Study-LLaMA-Open-and-Efﬁcient-Foundation-Language-Models/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="LLaMA-Open-and-Efﬁcient-Foundation-Language-Models"><a href="#LLaMA-Open-and-Efﬁcient-Foundation-Language-Models" class="headerlink" title="LLaMA Open and Efﬁcient Foundation Language Models"></a>LLaMA Open and Efﬁcient Foundation Language Models</h1><h2 id="📖-Abstract"><a href="#📖-Abstract" class="headerlink" title="📖 Abstract"></a>📖 Abstract</h2><p>We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.</p>
<span id="more"></span>

<h2 id="🎯-Goal"><a href="#🎯-Goal" class="headerlink" title="🎯 Goal"></a>🎯 Goal</h2><ul>
<li>To train a series of language models that achieve the best possible performance at various inference budgets.</li>
</ul>
<h2 id="🤔-Hypothesis"><a href="#🤔-Hypothesis" class="headerlink" title="🤔 Hypothesis"></a>🤔 Hypothesis</h2><ul>
<li>Training relatively small language models with more data can yield better performance.</li>
</ul>
<h2 id="💡-Methodology"><a href="#💡-Methodology" class="headerlink" title="💡 Methodology"></a>💡 Methodology</h2><ul>
<li>Use only publicly available data. The entire training dataset contains 1.4T tokens after tokenization.  <img src="/images/llama/CleanShot2023-08-07at231107.png" width=500px></li>
<li>Tokenize the data with Byte-Pair Encoding (BPE) algorithm, with the implementation from <a target="_blank" rel="noopener" href="https://github.com/google/sentencepiece">SentencePiece</a>.</li>
<li>Each token is used only once during training, except Wikipedia and Books, which are trained for approximately 2 epochs.</li>
<li>Architecture-wise, their network is based on the Transformer architecture, with the following differences:<ul>
<li>Pre-normalization:<ul>
<li>Normalize the input of each transformer sub-layer instead of the output.</li>
<li>Use the RMSNorm normalizing function.</li>
</ul>
</li>
<li>SwiGLU activation function:<ul>
<li>Replace the ReLU with the SwiGLU activation function.</li>
<li>SwiGLU has trainable parameters $W$ and $V$, they scale down the size of the hidden units with a factor of $\frac{2}{3}$.</li>
</ul>
</li>
<li>Rotary Embedding<ul>
<li>Replace absolute positional embedding with Rotary Positional Embedding(RoPE)</li>
</ul>
</li>
</ul>
</li>
<li>Use AdamW optimizer with the following hyperparemeters:<ul>
<li>$\beta_1 &#x3D; 0.9$</li>
<li>$\beta_2 &#x3D; 0.95$</li>
<li>Use a cosine learning rate schedule, where the final learning rate equals 10% of the maximal learning rate.</li>
<li>Use weight decay of 0.1</li>
<li>Use gradient clipping of 1.0</li>
<li>use 2000 warmup steps</li>
<li>Varying batch size and learning rate   <img src="/images/llama/CleanShot2023-08-07at235133.png" width=500/></li>
<li>Use efficient implementation of the causal multi-head attention to reduce memory usage and runtime<ul>
<li>Use works from <em>Self-attention does not need O(n2) memory</em> and <em>Flashattention Fast and memory-efﬁcient exact attention with io-awareness</em></li>
<li>Available at the <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/xformers">xformers</a> library.</li>
</ul>
</li>
<li>Reducing activation recomputation in large transformer models</li>
</ul>
</li>
</ul>
<h2 id="📊-Result-s"><a href="#📊-Result-s" class="headerlink" title="📊 Result(s)"></a>📊 Result(s)</h2><h3 id="Training-Loss"><a href="#Training-Loss" class="headerlink" title="Training Loss"></a>Training Loss</h3><img src="/images/llama/CleanShot2023-08-08at001349.png" width=500/>

<h3 id="Evolution-of-Performance"><a href="#Evolution-of-Performance" class="headerlink" title="Evolution of Performance"></a>Evolution of Performance</h3><img src="/images/llama/CleanShot2023-08-08at002024.png" width=500/>

<h3 id="Zero-Shot-and-Few-Shot-Performance"><a href="#Zero-Shot-and-Few-Shot-Performance" class="headerlink" title="Zero-Shot and Few-Shot Performance"></a>Zero-Shot and Few-Shot Performance</h3><img src="/images/llama/CleanShot2023-08-08at001437.png" width=500/>

<img src="/images/llama/CleanShot2023-08-08at001547.png" width=500/>

<img src="/images/llama/CleanShot2023-08-08at001621.png" width=500/>

<img src="/images/llama/CleanShot2023-08-08at001639.png" width=500/>

<img src="/images/llama/CleanShot2023-08-08at001710.png" width=500/>

<img src="/images/llama/CleanShot2023-08-08at001740.png" width=500/>

<img src="/images/llama/CleanShot2023-08-08at001831.png" width=500/>

<h3 id="Instruction-Finetuning-Result"><a href="#Instruction-Finetuning-Result" class="headerlink" title="Instruction Finetuning Result"></a>Instruction Finetuning Result</h3><img src="/images/llama/CleanShot2023-08-08at001912.png" width=500/>

<h3 id="Bias-Toxicity-and-Misinformation"><a href="#Bias-Toxicity-and-Misinformation" class="headerlink" title="Bias, Toxicity and Misinformation"></a>Bias, Toxicity and Misinformation</h3><img src="/images/llama/CleanShot2023-08-08at002122.png" width=500/>

<img src="/images/llama/CleanShot2023-08-08at002142.png" width=500/>

<img src="/images/llama/CleanShot2023-08-08at002204.png" width=500/>

<img src="/images/llama/CleanShot2023-08-08at002223.png" width=500/>

<h2 id="🔑-Summary-of-Key-Points"><a href="#🔑-Summary-of-Key-Points" class="headerlink" title="🔑 Summary of Key Points"></a>🔑 Summary of Key Points</h2><ul>
<li>Training with more data increases model Performance</li>
</ul>
<h2 id="❗-Significance"><a href="#❗-Significance" class="headerlink" title="❗ Significance"></a>❗ Significance</h2><ul>
<li>Smaller models can perform better than a large language model by scaling up training data, drastically reducing the inference cost</li>
</ul>
<h2 id="🙋‍♂️-Questions-and-Answers"><a href="#🙋‍♂️-Questions-and-Answers" class="headerlink" title="🙋‍♂️ Questions and Answers"></a>🙋‍♂️ Questions and Answers</h2><p><strong>Q</strong>: Can you briefly summarize the key contributions of the LLaMA paper?</p>
<p><strong>A</strong>: The LLaMA paper introduces a series of large language models ranging from 7B to 65B parameters. The key contributions are:</p>
<ul>
<li>Training large transformer models using publicly available data sources without relying on proprietary datasets. This makes the work more aligned with open sourcing.</li>
<li>Showing competitive performance can be achieved by scaling up training data, even for smaller models. For example, their 13B parameter model outperforms the 175B parameter GPT-3 on many benchmarks.</li>
<li>Releasing the pre-trained models openly to the research community. This will help democratize access to large language models for research.</li>
</ul>
<p><strong>Q</strong>: What was the motivation behind training the LLaMA models? How does this differ from previous work?</p>
<p><strong>A</strong>: Most prior work focused on training the largest models possible for a given computing budget. LLaMA instead optimized for inference efficiency - training smaller models that can reach a target performance level with less inference cost. This is important since inference cost is a bottleneck when deploying models.</p>
<p><strong>Q</strong>: Can you describe the model architecture, training data, and optimization process?</p>
<p><strong>A</strong>: The LLaMA models are transformer networks incorporating improvements like pre-layer normalization and rotary embeddings. They were trained on a mixture of CommonCrawl, Wikipedia, Books, GitHub code, etc., amounting to 1-1.4 trillion tokens. Optimization was done with AdamW and cosine decay learning rates. Efficiency techniques like activation checkpointing were used.</p>
<p><strong>Q</strong>: How did the LLaMA models compare to prior work on benchmark evaluations like GPT-3 and PaLM?</p>
<p><strong>A</strong>: LLaMA-13B outperformed GPT-3 on most benchmarks despite being 10x smaller. LLaMA-65B was competitive and often better than similarly sized models like Chinchilla-70B and PaLM-540B. This demonstrates their approach of scaling data over model size can work.</p>
<p><strong>Q</strong>: What limitations or potential risks did the authors describe for large language models like LLaMA?</p>
<p><strong>A</strong>: They analyzed model biases and toxicity using probes like RealToxicityPrompts, finding issues that need more research. They also reported on the carbon emissions from training, which were quite high. More work is needed to address these problems.</p>
<p><strong>Q</strong>: What directions for future work did the authors suggest?</p>
<p><strong>A</strong>: They plan to release larger models trained on more data since performance kept improving with their scaling approach. They also want to investigate instruction tuning further as a way to improve abilities rapidly.</p>
<p><strong>Q</strong>: How does the performance of LLaMA models evolve during training? Are there any interesting trends or observations?</p>
<p><strong>A</strong>: The paper showed performance on benchmarks improved steadily over training, correlating with reductions in training perplexity. One observation was performance on WinoGrande didn’t improve much between the 33B and 65B models, suggesting potential issues with that benchmark.</p>
<p><strong>Q</strong>: The authors propose training compute-optimal models that minimize inference costs. Can you expand on the benefits of this approach?</p>
<p><strong>A</strong>: Most prior work focused only on minimizing training costs for a target performance level. But inference cost is also critical when deploying models. The LLaMA approach optimizes inference by training smaller models on more data to reach the target performance at a lower inference cost.</p>
<p><strong>Q</strong>: Were the LLaMA models fine-tuned in any way? If not, how was strong performance achieved?</p>
<p><strong>A</strong>: No finetuning was done except for one instruction-tuned variant called LLaMA-I. The models performed well on many NLP benchmarks from pretraining on the large and diverse dataset. This demonstrates the generalization ability of the scaling approach.</p>
<p><strong>Q</strong>: The authors mention using techniques like activation checkpointing to improve training efficiency. Can you explain what this involves?</p>
<p><strong>A</strong>: Activation checkpointing reduces the memory and computing needed during training by avoiding recomputing certain activations during the backward pass. This is done by saving only the necessary activations from the forward pass and releasing unused activations from memory.</p>
<p><strong>Q</strong>: How did the authors evaluate model toxicity? What conclusions did they draw from this analysis?</p>
<p><strong>A</strong>: They tested toxicity using RealToxicityPrompts by generating completions and scoring them with Perspective API. Toxicity increased with model size, especially on respectful prompts, indicating larger models can be more toxic. However, the evaluation methodology had limitations.</p>
<p><strong>Q</strong>: What types of NLP tasks or applications do you think LLaMA models would suit well? Are there tasks where other models may be more appropriate?</p>
<p><strong>A</strong>: LLaMA models could be useful for open domain dialog, question answering, summarization, and other text generation applications. Their strong language modeling and few-shot performance suggest versatility. For very specialized domains like biomedical, other models pretrained on domain text may be better.</p>
<p><strong>Q</strong>: The authors propose training even larger LLaMA models. What potential concerns or limitations would this raise?</p>
<p><strong>A</strong>: Larger models increase risks around safety, bias, toxicity, and misuse. The compute and carbon cost also scales up. Deployment costs grow as well. So, while performance may improve further, there are many non-technical factors to consider before scaling up model size even more.</p>
<p><strong>Q</strong>: What are some key takeaways from this work that could guide future research on large language models?</p>
<p><strong>A</strong>: Pretraining on diverse public data can remove the need for private datasets while achieving strong performance. Compute-optimal models can minimize inference costs. Many challenges around responsible LLM development still require more research and solutions.</p>
<p><strong>Q</strong>: Were the LLaMA models tested in a chatbot or conversational agent setting? If not, how do you think they would perform such tasks?</p>
<p><strong>A</strong>: The paper evaluated the models on NLP benchmarks but not conversational tasks. However, their strong language modeling and few-shot abilities suggest they could perform well in open-ended dialog. Testing on conversational benchmarks could be an interesting direction for future work.</p>
<p><strong>Q</strong>: How could we improve diversity, mitigate bias, and address other ethical risks if we deployed these models in products and services?</p>
<p><strong>A</strong>: Many bias mitigation research is needed - techniques like adversarial debiasing, personalized bias ratings, censoring toxic generations, etc. Diversifying and filtering the training data can help. Extensive testing for different biases is required. There should be a focus on ethics and inclusivity throughout the development process.</p>
<p><strong>Q</strong>: What do you think could be promising directions for future work on more robust and beneficial language models?</p>
<p><strong>A</strong>: Aligning models with human values through techniques like instruction following and reinforcement learning from feedback seems promising. Expanding capabilities beyond text, like reasoning, causality, and symbol manipulation, could make models more generally intelligent. Building oversight, auditability, and control mechanisms will be important. There are many open challenges to work on.</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
              <a href="/tags/Paper-Study/" rel="tag"># Paper Study</a>
              <a href="/tags/Large-Language-Model/" rel="tag"># Large Language Model</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/03/01/Proximal-Policy-Optimization-Algorithms-PPO-and-PPO2/" rel="prev" title="Proximal Policy Optimization Algorithms (PPO and PPO2)">
                  <i class="fa fa-angle-left"></i> Proximal Policy Optimization Algorithms (PPO and PPO2)
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






    
  <div class="comments" id="disqus_thread">
    <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
  </div>
  
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2023</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Mun Hou</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/munhouiani" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="disqus" type="application/json">{"enable":true,"shortname":"munhousblog","count":true,"i18n":{"disqus":"disqus"}}</script>
<script src="/js/third-party/comments/disqus.js"></script>

</body>
</html>
